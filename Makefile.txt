This document explains how the Makefile supplied can be used to ease
building and testing the compiler. It was prepared by Kristis Makris 
a past TA for the course.


Why automate building ?
-----------------------

Compiling complex, large software, such as a compiler, can be a time
consuming and error prone task if done manually. It is to a
programmers best interest to automate the build process, especially
since it may combine executing multiple tools to compile a compiler
(lex/yacc/gcc) and additional tools (runtest/expect/diff/shell) to
test that the compiler built meets the desired quality standards.



Defining the build process
--------------------------

Building can become as simple as:

$ make

To achieve this level of automation, a programmer defines how the
source code can be compiled, and in what sequence, in the file
'Makefile'. Makefile "rules" are defined with a colon ":" as a
postfix. The first rule in the Makefile is the first to execute when a
programmer runs "make".

Lets examine the first rule in our Makefile:

all:    yacc lex $(OBJECTS) tests_prepare
	$(CC) -o $(BINARY) $(CFLAGS) y.tab.c lex.yy.c $(OBJECTS)

The definitions right next to ":" describe additional rules that need
to be first executed before rule "all" is executed. In this example,
building the compiler first requires running the "yacc" rule to
prepare the grammar source file with YACC, the "lex" rule to prepare
the tokenizer source file with lex, compiling the various source
files, and preparing an automatically generated portion of the
compiler testsuite. After all these additional rules are executed and
finish succesfully, the compiler we are building is compiled with a
command that links all object files together.

When "make" encounters an error in the build process, such as failing
to compile a source file (the compiler returns non-zero), it stops
immediately. Sometimes a user may not want "make" to stop, in which
case they prefix a command with a dash "-". For example, part of the
clean rule reads:

clean:
	-rm -f core $(BINARY) out.c *~
	-rm -f lex.yy.c y.tab.c y.tab.h y.tab.h.tmp y.output

The dash "-" in front of the "rm" command means if removing the files
requested failed, perhaps because they have already been removed and
don't exist, don't stop: continue executing the remaining commands in
this rule and remove other files that may have not been removed.

Defining "make" rules can become complicated when one attempts to
understand the various built-in rules provided by "make". A general
rule often written, which our Makefile uses is:

.c.o:
	$(CC) -c $(CFLAGS) $<

This rule can seem complicated and uses the "$<" built-in "make"
symbol. The rule means all C files should be compiled to object
files. Keeping a Makefile simple can be more valuable in a project of
this size rather than automating and optimizing the build process,
including automatic determination of dependencies, described
next. Some essential things to note though are:

- Comments in a Makefile are prefixed with a hash "#" character.

- CC and CFLAGS are global variables commonly used in a Makefile. For
  example, one can redefine CFLAGS to compile their compiler with
  additional options: enabling debugging with gdb (-g parameter),
  report more warnings (adding the -Wall argument), enabling
  optimizations, etc.

- Using global variables when defining rules is a good idea.

- It is possible to define rule macros.



Compiling again
---------------

Ideally, a programmer would like to wait as little as possible when
she compiles a program. A very good build process is one that can
automatically determine which source files the programmer modified
since the last time the program was compiled, compile only those
source files, and re-link everything. Such a process can essentially
automatically determine dependecies between source files: if one
header file is modified, automatically determine all source files that
#include that header file and compile all those source files.

The existing build process is not very good. It does not invoke
additional tools that can automatically determine build
dependencies. To guarantee safe recompilation one needs to first
remove all automatically generated files and recompile:

$ make clean; make



Testing
-------

Testing the compiler can be accomplished by running:

$ make test

This invokes the "test" rule that can invoke additional rules, such as
"tests_semantic" for semantic analysis testing, "tests_opt" to test if
optimizations have been implemented as expected, "tests_gen" to verify
the output generated by the built compiler produces, when executed,
the expected result, etc. For example, if one wanted to only run the
semantic analysis tests they would run:

$ make tests_semantic



Extending the testsuite
-----------------------

Testing relies on a collection of tools. It uses the DejaGNU testing
framework (the runtest command) which can implement a testsuite in
Expect, a language extension to Tcl/Tk. A minimal testsuite has been
written which, at the aim of simplifying testing, uses Expect only to
execute shell scripts. The core of the testing suite is thus written
as a shell script library, tests_lib.inc.

To add a new testing rule, such as testing if optimizations have been
implemented as expected, one needs to follow the example of how the
semantic analysis tests were written:

- Add a new rule, e.g. "tests_opt" in the Makefile that can invoke the
  runtest command with a new Expect file supplied,
  e.g. tests_opt.exp. Essentially copy-paste and modify accordingly
  the tests_semantic rule.

- Begin adding test cases in tests_opt.exp that call shell
  scripts. For example, add the test case "tests_opt/test_while_do.sh"
  and the Pascal source file "tests_opt/test_while_do.p"

- Define the expected result of each test case in a
  file. e.g. "tests_opt/test_while_do.p.expected"

Now, to test the optimization tests one needs to run:

$ make tests_opt

Or, to test an isolated test case:

$ cd tests_opt
$ ./test_while_do.sh


It is easy to envision things getting out of hand when multiple test
cases are defined. A programmer may get confused about which test
cases she added, which ones she removed, which expected result files
are correct, which have been incorrectly modified before and should be
changed back to their last known state, etc. The same, of course,
holds true of the development process itself. It is __HIGHLY__
recommended that programmers track their development within a shared
version control system, such as Subversion.

The testing framework can also be very powerful in detecting memory
errors. For example, instead of calling just the built compiler binary
"opc" one could instead call a memory leak monitoring tool inside
tests_lib.inc, function test_diff(). e.g. instead of calling:

$BINARY $FLAGS < $SOURCE_FILE > $SOURCE_FILE.output 2>&1

they could call:

mpatrol -g -i -C --dynamic --leak-table --alloc-byte=0x66 --oflow-byte=0x79 --oflow-size=64 $BINARY $FLAGS < $SOURCE_FILE > $SOURCE_FILE.output 2>&1

This would let them know for which test cases they may have memory
leak errors, freeing NULL pointers, incorrect pointer dereferences,
possible causes of crashes, etc. 

